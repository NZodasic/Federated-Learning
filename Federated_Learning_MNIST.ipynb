{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, Dataset"
      ],
      "metadata": {
        "id": "dRBpZ8HfbZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils: data partition (Dirichlet)\n",
        "def partition_dirichlet(labels, n_clients, alpha=0.5, rng=None):\n",
        "    \"\"\"\n",
        "    Partition indices by Dirichlet distribution to create label-skew (non-IID).\n",
        "    Returns a list of index lists, one per client.\n",
        "    - labels: numpy array of labels for full dataset\n",
        "    - n_clients: number of clients\n",
        "    - alpha: Dirichlet concentration (smaller -> more skewed)\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.RandomState(0)\n",
        "    n_classes = labels.max() + 1\n",
        "    idx_by_class = [np.where(labels == c)[0] for c in range(n_classes)]\n",
        "    client_indices = [[] for _ in range(n_clients)]\n",
        "\n",
        "    # For each class, split its indices to clients via Dirichlet distribution\n",
        "    for c in range(n_classes):\n",
        "        n_c = len(idx_by_class[c])\n",
        "        # proportions for this class across clients\n",
        "        probs = rng.dirichlet([alpha] * n_clients)\n",
        "        # ensure no zero by tiny fix\n",
        "        probs = (probs / probs.sum()) * n_c\n",
        "        # floor -> get sizes that sum ~= n_c, assign remainder randomly\n",
        "        sizes = np.floor(probs).astype(int)\n",
        "        remainder = n_c - sizes.sum()\n",
        "        if remainder > 0:\n",
        "            # distribute the remainder to random clients (weighted)\n",
        "            add_idx = rng.choice(n_clients, remainder, replace=True)\n",
        "            for ai in add_idx:\n",
        "                sizes[ai] += 1\n",
        "        # now split and assign\n",
        "        shuffled = rng.permutation(idx_by_class[c])\n",
        "        pointer = 0\n",
        "        for client_id in range(n_clients):\n",
        "            cnt = sizes[client_id]\n",
        "            if cnt > 0:\n",
        "                sel = shuffled[pointer: pointer + cnt]\n",
        "                client_indices[client_id].extend(sel.tolist())\n",
        "                pointer += cnt\n",
        "    return client_indices"
      ],
      "metadata": {
        "id": "VXI34_E_bc4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Simple CNN model\n",
        "# -------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, padding=1),  # 28x28 -> 28x28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                 # -> 14x14\n",
        "            nn.Conv2d(16, 32, 3, padding=1), # -> 14x14\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                 # -> 7x7\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "BETOcBi2bfu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Local training routine\n",
        "# -------------------\n",
        "def local_train(model, dataloader, device, epochs=1, lr=0.01):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model.state_dict()"
      ],
      "metadata": {
        "id": "WJf2fLicbgu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Evaluate model on test set\n",
        "# -------------------\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0.0\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dataloader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss_sum += criterion(logits, yb).item()\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return loss_sum / total, correct / total"
      ],
      "metadata": {
        "id": "_62-IrphbigF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# FedAvg aggregation (weighted by n_samples)\n",
        "# -------------------\n",
        "def fedavg_aggregate(global_state, local_states, local_sizes):\n",
        "    new_state = {}\n",
        "    total = sum(local_sizes)\n",
        "    # iterate params\n",
        "    for key in global_state.keys():\n",
        "        accum = None\n",
        "        for st, n in zip(local_states, local_sizes):\n",
        "            w = st[key].float() * (n / total)\n",
        "            if accum is None:\n",
        "                accum = w.clone()\n",
        "            else:\n",
        "                accum += w\n",
        "        new_state[key] = accum\n",
        "    return new_state"
      ],
      "metadata": {
        "id": "Xk3KpEp2bjvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkDjun0nbVrw"
      },
      "outputs": [],
      "source": [
        "# -------------------\n",
        "# Main simulation\n",
        "# -------------------\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Hyperparams\n",
        "    n_clients = 5\n",
        "    alpha = 0.5           # Dirichlet concentration (smaller -> more skewed/non-IID)\n",
        "    rounds = 30\n",
        "    local_epochs = 1\n",
        "    local_batch = 64\n",
        "    lr = 0.05\n",
        "\n",
        "    # MNIST transform / datasets\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    mnist_test  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # prepare labels array for partitioning\n",
        "    labels = np.array(mnist_train.targets)\n",
        "    client_indices = partition_dirichlet(labels, n_clients=n_clients, alpha=alpha, rng=np.random.RandomState(42))\n",
        "    # Create DataLoaders for each client\n",
        "    client_loaders = []\n",
        "    client_sizes = []\n",
        "    for idxs in client_indices:\n",
        "        sub = Subset(mnist_train, idxs)\n",
        "        ld = DataLoader(sub, batch_size=local_batch, shuffle=True, num_workers=0)\n",
        "        client_loaders.append(ld)\n",
        "        client_sizes.append(len(idxs))\n",
        "    print(\"Client sizes:\", client_sizes)\n",
        "\n",
        "    # global test loader\n",
        "    test_loader = DataLoader(mnist_test, batch_size=512, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Initialize global model\n",
        "    global_model = SimpleCNN().to(device)\n",
        "    global_state = global_model.state_dict()\n",
        "\n",
        "    # Training rounds\n",
        "    hist = {\"round\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "    for r in range(1, rounds + 1):\n",
        "        local_states = []\n",
        "        # Each client trains locally starting from global_state\n",
        "        for cid in range(n_clients):\n",
        "            # create local model copy and load global weights\n",
        "            local_model = SimpleCNN().to(device)\n",
        "            local_model.load_state_dict(global_state)\n",
        "            # train locally\n",
        "            st = local_train(local_model, client_loaders[cid], device, epochs=local_epochs, lr=lr)\n",
        "            local_states.append({k: v.cpu() for k, v in st.items()})  # move to cpu for aggregation\n",
        "        # Aggregation (FedAvg)\n",
        "        aggregated = fedavg_aggregate(global_state, local_states, client_sizes)\n",
        "        # Update global_state with aggregated weights\n",
        "        # replace numeric tensors in global_state\n",
        "        for k in global_state.keys():\n",
        "            global_state[k] = aggregated[k]\n",
        "        global_model.load_state_dict(global_state)\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc = evaluate(global_model, test_loader, device)\n",
        "        hist[\"round\"].append(r)\n",
        "        hist[\"test_loss\"].append(test_loss)\n",
        "        hist[\"test_acc\"].append(test_acc)\n",
        "        print(f\"Round {r:02d} | Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")\n",
        "    print(\"Done. Final test acc:\", hist[\"test_acc\"][-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "LKeH3HHabnbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}